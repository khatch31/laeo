{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "visualization script.ipynb",
      "provenance": [
        {
          "file_id": "1om0pY4JfPQXzZHCIl7wnOChHfbYhRsfo",
          "timestamp": 1658855827303
        }
      ],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu5P61Lh3t6M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import dill\n",
        "from contrastive_rl import contrastive\n",
        "from contrastive_rl.sac import utils as contrastive_utils\n",
        "from acme import specs\n",
        "import jax\n",
        "import numpy as np\n",
        "\n",
        "import video #<-- This is an internal google library, but imageio should be able to do similar things\n",
        "from base64 import b64encode\n",
        "from IPython.display import display, HTML\n",
        "import tempfile\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as jnp\n",
        "import gym\n",
        "from acme.wrappers import gym_wrapper\n",
        "\n",
        "import metaworld\n",
        "from contrastive_rl import classifier_envs\n",
        "from acme.wrappers import gym_wrapper\n",
        "from acme.wrappers import step_limit\n",
        "from contrastive_rl import classifier_utils\n",
        "from metaworld.envs.mujoco.utils import rotation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/eysenbach/acme/36766008/1/checkpoints/learner'  # <-- Of course, this will be different for you\n",
        "reader = tf.train.load_checkpoint(filename)\n",
        "params = reader.get_tensor('learner/.ATTRIBUTES/py_state')\n",
        "state = dill.loads(params)"
      ],
      "metadata": {
        "id": "yI8zuom2jF4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, obs_dim = contrastive_utils.make_environment('sawyer_drawer_image_corner3', start_index=0, end_index=-1, seed=0,\n",
        "                                          frame_stack=False)\n",
        "spec = specs.make_environment_spec(env)\n",
        "networks = sac.make_networks(spec, obs_dim,\n",
        "                             actor_min_std=1e-6,  # <-- Make sure this is the same value used for training.\n",
        "                             use_image_obs=True)\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "@jax.jit\n",
        "def _act(obs, key):\n",
        "  dist_params = networks.policy_network.apply(\n",
        "    state.policy_params, obs[None])\n",
        "  key, rng = jax.random.split(key, 2)\n",
        "  # action = dist_params.mode()  # <-- Use this if you want to act deterministically\n",
        "  action = networks.sample(dist_params, rng)\n",
        "  return action[0], key\n",
        "\n",
        "ts = env.reset()\n",
        "_act(ts.observation, key)"
      ],
      "metadata": {
        "id": "pK2gNZ4AjH1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the video writer commands will have to change here. I'd probably recommend using\n",
        "# imageio. You just make a list of all the images, and then call imageio.mimsave(filename, image_list)\n",
        "\n",
        "for _ in range(10):\n",
        "  video_filename = '/tmp/video.mp4'\n",
        "  writer = video.VideoWriter(video_filename, frame_rate=10, video_format='mp4')\n",
        "\n",
        "  vec = []\n",
        "  ts = env.reset()\n",
        "  env._goal_img = ts.observation.copy()\n",
        "  for t in tqdm.trange(50):\n",
        "    a, key = _act(ts.observation, key)\n",
        "    ts = env.step(a.to_py())\n",
        "    # use the following lines for image-based environments.\n",
        "    s = ts.observation[:obs_dim]\n",
        "    g = ts.observation[obs_dim:]\n",
        "    img = np.concatenate([\n",
        "      s.reshape((64, 64, 3)),\n",
        "      g.reshape((64, 64, 3))\n",
        "    ], axis=1)\n",
        "    # for state-based environments, use the following\n",
        "    # img = env.render(mode='rgb_array'), or something like that\n",
        "    writer.add(img)\n",
        "    if ts.last():\n",
        "      break\n",
        "  writer.close()\n",
        "\n",
        "  mp4 = open(video_filename,'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  html = HTML(\"\"\"\n",
        "  <video width=800 framerate=1 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url)\n",
        "  display(html)\n",
        "  plt.plot(env.environment.environment.environment._dist)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7nuWxhIYjPPv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
